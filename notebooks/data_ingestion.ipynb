{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion Pipeline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import glob, os, shutil\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write code that creates a list of Android Apps on \n",
    "#apkpure that will make up the benign apps in your training set. \n",
    "def get_submap_xmls(sitemap):\n",
    "    resp = requests.get(sitemap)\n",
    "    soup = BeautifulSoup(resp.content, 'xml')\n",
    "    url = soup.find_all('loc') \n",
    "    result = []\n",
    "    for link in url:\n",
    "        result += [link.get_text()]\n",
    "    return result\n",
    "\n",
    "def cat_lst(link_lst):\n",
    "    '''\n",
    "    Get a list of categories by scraping xml list from sitemap.xml 'https://apkpure.com/sitemap.xml'\n",
    "    '''\n",
    "    cat = []\n",
    "    reg = '(?<=sitemaps\\/)(.*)(?=\\-\\d)|(?<=sitemaps\\/)(.*)(?=\\.xml)'\n",
    "    for xml in link_lst:\n",
    "        cat += [re.search(reg,xml).groups()[1]]\n",
    "    return [i for i in cat if i]\n",
    "\n",
    "def sample_by_cat(categories):\n",
    "    '''\n",
    "    Given categories, find the soup of all decompressed category gz files\n",
    "    '''\n",
    "    soups = []\n",
    "    for c in categories:\n",
    "        url = 'https://apkpure.com/sitemaps/{}.xml.gz'.format(c)\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "        except:\n",
    "            url = 'https://apkpure.com/sitemaps/{}.xml.gz'.format(c+'-1')\n",
    "            r = requests.get(url)\n",
    "    \n",
    "        data = gzip.decompress(r.content)\n",
    "        soup = BeautifulSoup(data,features = 'lxml')\n",
    "        soups.append(soup)\n",
    "    return soups\n",
    "\n",
    "def get_app_urls(sitemap, cat, number):\n",
    "    '''\n",
    "    Obtain the a selective number of download links for apps by category\n",
    "    '''\n",
    "    xmls = get_submap_xmls(sitemap)\n",
    "    \n",
    "    if cat == 'all':\n",
    "        categories = cat_lst(xmls)\n",
    "    elif type(cat) == int:\n",
    "        categories = random.choices(cat_lst(xmls), k = cat)\n",
    "    else:\n",
    "        categories = cat\n",
    "        \n",
    "    soups = sample_by_cat(categories)\n",
    "    apps = []\n",
    "    for soup in soups:\n",
    "        count = 0\n",
    "        sp = soup.find_all(re.compile('loc')) \n",
    "        lst = [] \n",
    "        for i in sp:\n",
    "            if re.match('<loc>', str(i)) and count < number:\n",
    "                try:\n",
    "                    lst += [re.search('(?<=<loc>)(https:\\/\\/apkpure.com\\/.*?\\/.*[a-zA-Z\\d].*)(?=<\\/loc>)', str(i)).group()] #find all urls storec in loc\n",
    "                    count += 1\n",
    "                except:\n",
    "                    continue\n",
    "        apps += lst\n",
    "    return apps\n",
    "\n",
    "#Given an android app on apkpure, download the apk, \n",
    "#decompile the apk to Smali code.\n",
    "\n",
    "def download_link(app_link, outpath, cat):\n",
    "    '''\n",
    "    From the app link, find the download page, obtain the download link\n",
    "    '''\n",
    "    if not os.path.exists(outpath):\n",
    "        os.mkdir(outpath)\n",
    "    if not os.path.exists(outpath + '/' + cat):\n",
    "        os.mkdir(outpath + '/' + cat)\n",
    "\n",
    "    for url in app_link:\n",
    "        download_link = url + '/download?from=details'\n",
    "        r1 = requests.get(download_link)\n",
    "        soup = BeautifulSoup(r1.text)\n",
    "        try:\n",
    "            download_link_file = soup.find('div',attrs = {\"class\":\"fast-download-box fast-bottom\"}).find('p',attrs = {'class':'down-click'}).find('a',href = True)['href']\n",
    "        except:\n",
    "            continue\n",
    "        r2 = requests.get(download_link_file)\n",
    "        apkfile = r2.content\n",
    "        complete_name = os.path.join(outpath+'/'+cat+'/', url.split('/')[-1]+\".apk\")\n",
    "        out_name = os.path.join(outpath+'/'+cat+'/', url.split('/')[-1])\n",
    "        with open(complete_name, 'wb') as fh:\n",
    "            fh.write(apkfile)\n",
    "        subprocess.call(['apktool', 'd', outpath+'/'+cat+'/'+url.split('/')[-1]+\".apk\", '-o',out_name])\n",
    "    \n",
    "#Given a directory contain Smali code (as returned from running apktool), \n",
    "#organize it on disk\n",
    "def clean_folder(app_path):\n",
    "    if '.DS_Store' not in app_path:\n",
    "        if os.path.isdir(app_path):\n",
    "            subs = os.listdir(app_path)\n",
    "            for s in subs:\n",
    "                if s not in ['smali', 'AndroidManifest.xml']:\n",
    "                    path = app_path+'/'+s\n",
    "                    if os.path.isdir(path):\n",
    "                        shutil.rmtree(path)\n",
    "                    elif os.path.isfile(path):\n",
    "                        os.remove(path)\n",
    "        else:\n",
    "            os.remove(app_path)\n",
    "            \n",
    "def clean_disk(out):\n",
    "    '''\n",
    "    keep only smali folder and AndroidManifest.xml\n",
    "    '''\n",
    "    subs = os.listdir(out)\n",
    "    for s in subs:\n",
    "        if os.path.isdir(out + '/' + s):\n",
    "            files = os.listdir(out + '/' + s)\n",
    "            for fi in files:\n",
    "                clean_folder(out + '/' + s + '/' + fi)\n",
    "        else:\n",
    "            os.remove(out + '/' + s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "appurl = get_app_urls(\"https://apkpure.com/sitemap.xml\", ['beauty', 'communication'] , 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_link(appurl[:20], 'data', 'beauty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_link(appurl[20:], 'data', 'communication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_disk('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
